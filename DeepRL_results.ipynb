{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "222115e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C, SAC, DDPG\n",
    "from src.environment.DiscreteBattery import DiscreteBattery\n",
    "from src.agent.utils import get_train_test_sets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131024d",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83426c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 300\n",
    "N_test = 100\n",
    "test_env = DiscreteBattery(discrete_action_space=True, days=list(range(N_train)))\n",
    "train_Ppvs, train_Pconsos, test_Ppvs, test_Pconsos = get_train_test_sets(test_env, N_train=N_train, N_test=N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93055f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.998704248999985\n",
      "39.61742491999998\n",
      "39.284732129999995\n",
      "40.17158335966666\n",
      "39.014186534000004\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = DQN.load(f\"cleps/dqn_lr_1e-3_eps1e-1_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "137ed6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.76508645099998\n",
      "40.79360924466665\n",
      "38.91041946366666\n",
      "39.76417003333332\n",
      "39.37478524466666\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = DQN.load(f\"cleps/dqn_lr_1e-3_eps2e-1_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8c3d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.36708730766666\n",
      "38.85296593033333\n",
      "39.60588589699999\n",
      "39.830707800333315\n",
      "39.619948281999996\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = DQN.load(f\"cleps/dqn_lr_1e-4_eps1e-1_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73fb4d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.332106533333324\n",
      "38.80476589166666\n",
      "39.2697211\n",
      "39.29970959299999\n",
      "39.56055886666665\n"
     ]
    }
   ],
   "source": [
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = DQN.load(f\"cleps/dqn_lr_1e-4_eps2e-1_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0720d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.load(\"/Users/luweber/Projets/clean-battery/data/experiments/bill_min/agent_003/PI-q-table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f1fb5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.191714058666655\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(100)\n",
    "for day in range(N_test):\n",
    "    test_env.Ppv = test_Ppvs[day]\n",
    "    test_env.Pconso = test_Pconsos[day]\n",
    "    obs, info = test_env.reset(change=False)\n",
    "    \n",
    "    cumulated_reward = 0\n",
    "    for h in range(143):\n",
    "        X = obs['SOC']\n",
    "        T = obs['time']\n",
    "        D = obs['delta']\n",
    "#         states[h] = X / 100\n",
    "        action = q_table[T, D, X].argmin()\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        cumulated_reward += reward\n",
    "    rewards[day] = -cumulated_reward\n",
    "print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "903b7927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.61717133333332\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(100)\n",
    "for day in range(N_test):\n",
    "    test_env.Ppv = test_Ppvs[day]\n",
    "    test_env.Pconso = test_Pconsos[day]\n",
    "    obs, info = test_env.reset(change=False)\n",
    "    \n",
    "    cumulated_reward = 0\n",
    "    for h in range(143):\n",
    "        X = obs['SOC']\n",
    "        T = obs['time']\n",
    "        D = obs['delta']\n",
    "#         states[h] = X / 100\n",
    "        action = 20\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        cumulated_reward += reward\n",
    "    rewards[day] = -cumulated_reward\n",
    "print(rewards.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a205ed1",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d7bdce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.31138291175564\n",
      "43.330022547376586\n",
      "41.71753078193807\n",
      "41.64914796439629\n",
      "42.56744387098881\n"
     ]
    }
   ],
   "source": [
    "test_env = DiscreteBattery(discrete_action_space=False)\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = PPO.load(f\"cleps/ppo_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c63f11",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d44e941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.61717133333332\n",
      "43.29683799999998\n",
      "45.75025808884283\n",
      "43.31271797662972\n",
      "44.04301290750703\n"
     ]
    }
   ],
   "source": [
    "test_env = DiscreteBattery(discrete_action_space=False)\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = A2C.load(f\"cleps/a2c_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a60d2",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2254d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.77742388659553\n",
      "39.380497879414044\n",
      "39.36470531048472\n",
      "39.01615124268644\n",
      "39.391096494162895\n"
     ]
    }
   ],
   "source": [
    "test_env = DiscreteBattery(discrete_action_space=False)\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = SAC.load(f\"cleps/sac_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81488bfe",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdd989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.482393720716644\n",
      "39.70285278919432\n",
      "39.24226006943459\n",
      "39.246914450468005\n",
      "39.418292223957636\n"
     ]
    }
   ],
   "source": [
    "test_env = DiscreteBattery(discrete_action_space=False)\n",
    "for seed in [0, 1, 2, 3, 4]:\n",
    "    model = DDPG.load(f\"cleps/ddpg_{seed}\")\n",
    "    rewards = np.zeros(100)\n",
    "    for day in range(N_test):\n",
    "        test_env.Ppv = test_Ppvs[day]\n",
    "        test_env.Pconso = test_Pconsos[day]\n",
    "        obs, info = test_env.reset(change=False)\n",
    "\n",
    "        cumulated_reward = 0        \n",
    "        for h in range(143):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "            cumulated_reward += reward\n",
    "        rewards[day] = -cumulated_reward\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d8d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
